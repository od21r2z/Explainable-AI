{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899bb7ec-ea60-4b92-a8f1-167ab64ec251",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "This is a Jupyter Notebook to explain the functions used in the local / global explainability master thesis.\n",
    "\n",
    "Author: Roman Zogg  \n",
    "Email: od21r2z@leeds.ac.uk \n",
    "Written: 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d7f7ae-12f0-4bca-8cfc-61f5d45daee5",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n",
    "The main standardlibraries are pandas, numpy, matplotlib and seaborn.  \n",
    "For the modelling Scikit Learn and XGBoost are used.  \n",
    "For SHAP and LIME the repsective libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6919bd4a-4014-4bd2-a040-9e002f97c90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "# Import the main libraries for the functions below\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, BayesianRidge\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from warnings import simplefilter, filterwarnings\n",
    "from collections import defaultdict\n",
    "import matplotlib as mpl\n",
    "import squarify\n",
    "import re\n",
    "import shap\n",
    "import lime\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f677e3-0504-4bfa-8103-d01fcc6cf766",
   "metadata": {},
   "source": [
    "## Function: `modelSelection`\n",
    "The function `modelSelection` evaluates various classification models based on recall, precision, and F1-score using stratified cross-validation. It visualizes the results using box plots for each metric.\n",
    "\n",
    "The code for this function has been inspired by:  \n",
    "Lukas Frei (https://towardsdatascience.com/detecting-credit-card-fraud-using-machine-learning-a3d83423d3b8)\n",
    "\n",
    "\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "- `X` : array-like or pd.DataFrame, shape (n_samples, n_features)\n",
    "    - Training data.\n",
    "<br> <br>\n",
    "- `y` : array-like or pd.Series, shape (n_samples)\n",
    "    - Target labels.\n",
    "  <br> <br>\n",
    "- `n_split` : int, optional, default=5\n",
    "    - Determines the number of folds in stratified cross-validation.\n",
    "  <br> <br>  \n",
    "- `random_state` : int or None, optional, default=42\n",
    "    - Random seed for reproducibility. \n",
    "    - None for random initialization.\n",
    "<br> <br>\n",
    "#### Outputs:\n",
    "\n",
    "- Visualizes box plots showing the performance of each model on recall, precision, and F1-score.\n",
    "- Saves the visualized plots as \"model_selection_plot.png\" at a high resolution (1200 dpi).\n",
    "\n",
    "#### Description:\n",
    "\n",
    "1. **Model Definitions**: The function evaluates the performance of several classification models. These models include:\n",
    "   - Logistic Regression\n",
    "   - Decision Tree Classifier\n",
    "   - Random Forest Classifier\n",
    "   - Neural Network (MLP Classifier)\n",
    "   - Gaussian Naive Bayes\n",
    "   - XGBoost Classifier\n",
    "   - Support Vector Classifier\n",
    "<br> <br>\n",
    "2. **Cross-Validation**: The function employs stratified cross-validation to account for imbalances in the target variable, which is important in classification tasks. The number of splits/folds in cross-validation is set by `n_split`.\n",
    "\n",
    "3. **Metric Evaluation**:\n",
    "    - Recall, precision, and F1-score for each model are computed.\n",
    "    - Results of these evaluations are stored in respective lists for visualization.\n",
    "<br><br>\n",
    "4. **Visualization**:\n",
    "    - Utilizes Matplotlib to visualize the performance metrics in the form of box plots.\n",
    "    - Generates separate box plots for recall, precision, and F1-score of the evaluated models.\n",
    "    - Customizes the appearance (e.g., font family, font size) for improved readability.\n",
    "<br><br>\n",
    "5. **Saving Visualization**:\n",
    "    - The created visualization is saved as an image with the filename \"model_selection_plot.png\".\n",
    "    - The image is saved with high resolution (1200 dpi) to preserve details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ddef91-7623-400c-bb44-a56a42d8f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "\n",
    "def modelSelection(X, y, n_split = 5, random_state = 42):\n",
    "    models = []\n",
    "    \n",
    "    # Defining the models\n",
    "    models.append(('LogReg', LogisticRegression(class_weight = 'balanced', max_iter = 200, random_state = random_state)))\n",
    "    models.append(('Tree', DecisionTreeClassifier(random_state = random_state)))\n",
    "    models.append(('RandForest', RandomForestClassifier(class_weight = 'balanced', random_state = random_state)))\n",
    "    models.append(('NeuralNet', MLPClassifier(solver = 'adam', hidden_layer_sizes=(120, 60, 10), max_iter=100, random_state = random_state)))\n",
    "    models.append(('NaiveBayes', GaussianNB()))\n",
    "    models.append(('XGB', XGBClassifier(random_state = random_state)))\n",
    "    #models.append(('KNearN', KNeighborsClassifier()))\n",
    "    models.append(('SuppVec', SVC(class_weight = 'balanced', random_state = random_state)))\n",
    " \n",
    "    \n",
    "    # Runing the Cross validation in Recall\n",
    "    recallResult = []\n",
    "    recallModelName = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        print(f\"Processing Recall for model: {name}\")  # Debug print\n",
    "        kFold = StratifiedKFold(n_splits = n_split, shuffle = True, random_state = random_state)\n",
    "        cvResults = cross_val_score(model, X, y, cv=kFold, scoring='recall')\n",
    "        recallResult.append(cvResults)\n",
    "        recallModelName.append(name)\n",
    "        \n",
    "    # Runing the Cross validation in F1\n",
    "    f1Result = []\n",
    "    f1ModelName = []\n",
    "        \n",
    "    for name, model in models:\n",
    "        print(f\"Processing F1 for model: {name}\")  # Debug print\n",
    "        kFold = StratifiedKFold(n_splits = n_split, shuffle = True, random_state = random_state)\n",
    "        cvResults = cross_val_score(model, X, y, cv=kFold, scoring='f1')\n",
    "        f1Result.append(cvResults)\n",
    "        f1ModelName.append(name)    \n",
    "     \n",
    "    # Runing the Cross validation in Precision\n",
    "    precisionResult = []\n",
    "    precisionModelName = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        print(f\"Processing Precision for model: {name}\")  # Debug print\n",
    "        kFold = StratifiedKFold(n_splits = n_split, shuffle = True, random_state = random_state)\n",
    "        cvResults = cross_val_score(model, X, y, cv=kFold, scoring='precision')\n",
    "        precisionResult.append(cvResults)\n",
    "        precisionModelName.append(name) \n",
    "    \n",
    "    # Setting the font\n",
    "    mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "    font_size = 13  # Change this value to adjust the font size\n",
    "    \n",
    "    # Plotting the results    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(11.8, 10), sharey=True)\n",
    "    \n",
    "    axes[0].boxplot(recallResult) \n",
    "    axes[0].set_title('Recall of Classification Algorithms', fontsize = font_size+2)\n",
    "    axes[0].set_ylabel('Recall', fontsize = font_size+1)\n",
    "    axes[0].set_xticklabels([])\n",
    "    \n",
    "    axes[1].boxplot(precisionResult)\n",
    "    axes[1].set_title('Precision of Classification Algorithms', fontsize = font_size+2)\n",
    "    axes[1].set_ylabel('Precision', fontsize = font_size+1)\n",
    "    axes[1].set_xticklabels([])\n",
    "    \n",
    "    axes[2].boxplot(f1Result)\n",
    "    axes[2].set_title('F1 of Classification Algorithms', fontsize = font_size+2)\n",
    "    axes[2].set_xlabel('Algorithm', fontsize = font_size+1)\n",
    "    axes[2].set_ylabel('F1', fontsize = font_size+1)\n",
    "    axes[2].set_xticklabels(f1ModelName, fontsize = font_size+1)\n",
    "    \n",
    "    fig.tight_layout(pad=3.0)\n",
    "    \n",
    "    \n",
    "    # Save the figure with 800 dpi\n",
    "    fig.savefig(\"model_selection_plot.png\", dpi=1200, bbox_inches='tight')\n",
    "    \n",
    "    # Show the plots\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab5b7b8-25b5-44a6-88e6-f6c2a2b4ca5c",
   "metadata": {},
   "source": [
    "## Function: `parameterTuning`\n",
    "Performs hyperparameter tuning using grid search on a given estimator using stratified cross-validation. The function also splits the data into training and test sets before conducting the grid search.\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "- `X` : array-like or pd.DataFrame, shape (n_samples, n_features)\n",
    "    - Training data.\n",
    "  <br> <br>\n",
    "- `y` : array-like or pd.Series, shape (n_samples)\n",
    "    - Target labels.\n",
    "<br> <br>\n",
    "- `paramGrid` : dict or list of dictionaries\n",
    "    - Dictionary with parameter names (string) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored.\n",
    "  <br> <br>\n",
    "- `estimator` : estimator object\n",
    "    - The estimator for which the hyperparameters are being tuned.\n",
    "  <br> <br>  \n",
    "- `split` : float, optional, default=0.75\n",
    "    - Represents the proportion of the data to include in the training set. \n",
    "    - Should be between 0.0 and 1.0.\n",
    "  <br> <br>\n",
    "- `scoring` : string, callable, dict or None, optional, default='f1'\n",
    "    - Scoring metric to evaluate the predictions on the test set. For the provided function, it's set to default as 'f1'.\n",
    "    - If single string: Evaluates the predictions on the test set using the provided single metric. Common options are 'precision', 'recall', 'f1', etc.\n",
    "    - If None, the estimator’s default scorer is used.\n",
    "    - For more details on valid scoring strings or callables, refer to Scikit-learn's [model evaluation documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter).\n",
    "   <br> <br> \n",
    "- `n_split` : int, optional, default=3\n",
    "    - Determines the number of folds in stratified cross-validation.\n",
    "<br> <br>\n",
    "- `random_state` : int or None, optional, default=42\n",
    "    - Random seed for reproducibility. \n",
    "    - None for random initialization.\n",
    "<br> <br>\n",
    "#### Returns:\n",
    "\n",
    "- `bestParams` : dict\n",
    "    - Dictionary containing the best hyperparameters found by the grid search.\n",
    "<br> <br>\n",
    "#### Description:\n",
    "\n",
    "1. **Data Splitting**: \n",
    "    - The function first splits the provided data (`X`, `y`) into training and test sets using the `train_test_split` method. \n",
    "    - It ensures that the split is stratified to maintain the proportion of target classes.\n",
    "  <br> <br>\n",
    "2. **Cross-Validation Setup**: \n",
    "    - A stratified cross-validation object is created to be used in the grid search. \n",
    "    - Stratified cross-validation ensures that each fold has the same proportion of observations with a given target value as the complete dataset.\n",
    "<br> <br>\n",
    "3. **Grid Search Setup**:\n",
    "    - A `GridSearchCV` object is initialized with the provided estimator, parameter grid, cross-validation object, and scoring metric.\n",
    "<br> <br>\n",
    "4. **Conducting Grid Search**:\n",
    "    - The grid search is performed on the training data. The search explores all combinations of parameters provided in the `paramGrid`.\n",
    "   <br> <br> \n",
    "5. **Result Retrieval**:\n",
    "    - After the grid search is complete, the best hyperparameters are extracted and printed.\n",
    "    - The best hyperparameters are returned by the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1f833-14e1-4492-80fe-a0ceae391083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a function for Hyperparameter tuning\n",
    "\n",
    "def parameterTuning (X, y, paramGrid, estimator, split = 0.75, scoring = 'f1', n_split = 3, random_state = 42):\n",
    "    \n",
    "    # Split the data set into test and train\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify = y, \n",
    "        train_size = split, \n",
    "        random_state = random_state)\n",
    "    \n",
    "    # Strafified Cross Validator\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits = n_split,\n",
    "        shuffle = True,\n",
    "        random_state = random_state\n",
    "    )\n",
    "    \n",
    "    # Creating Grid Search object\n",
    "    gridSearch = GridSearchCV(\n",
    "        estimator = estimator,\n",
    "        param_grid = paramGrid,\n",
    "        cv = cv,\n",
    "        scoring = scoring\n",
    "    )\n",
    "    \n",
    "    # Conduct search\n",
    "    gridResult = gridSearch.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best parameters\n",
    "    bestParams = gridResult.best_params_\n",
    "    \n",
    "    print(bestParams)\n",
    "    \n",
    "    return bestParams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b875c1-8965-4639-b647-9b98751ba38a",
   "metadata": {},
   "source": [
    "## Function: `confusionMatrix`\n",
    "\n",
    "The `confusionMatrix` function evaluates a given machine learning model's performance on a dataset by plotting its confusion matrix and printing some performance metrics, including Recall, Precision, and F1 Score.\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "- `X` : array-like, shape (n_samples, n_features)\n",
    "    - Feature matrix. The samples and set of features to be used for testing and training the model.\n",
    "<br><br>\n",
    "- `y` : array-like, shape (n_samples,)\n",
    "    - True labels for the samples in `X`.\n",
    "<br><br>\n",
    "- `model` : Scikit-learn classifier instance\n",
    "    - The machine learning model or classifier which needs to be evaluated. This should have `fit` and `predict` methods, typical of Scikit-learn models.\n",
    "<br><br>\n",
    "- `split` : float, optional, default=0.75\n",
    "    - Determines the proportion of the data to be used for training. The remainder will be used for testing. For instance, if split is 0.75, it means 75% of the data is used for training and the rest for testing.\n",
    "<br><br>\n",
    "- `random_state` : int, optional, default=42\n",
    "    - Determines the randomness of the train-test data splitting. Pass an int for reproducible output across multiple function calls.\n",
    "<br><br>\n",
    "#### Outputs:\n",
    "\n",
    "No explicit return. The function plots the confusion matrix for the classifier on the given data. It also prints the Recall, Precision, and F1 Score metrics to provide insights into the model's performance.\n",
    "\n",
    "#### Function Behavior:\n",
    "\n",
    "1. The function first splits the given data (`X`, `y`) into training and testing sets based on the provided `split` ratio.\n",
    "2. The model is trained or fitted using the training data.\n",
    "3. The trained model is then used to predict the labels for the test data.\n",
    "4. A confusion matrix is plotted using the true labels and the predicted labels for the test data.\n",
    "5. The Recall, Precision, and F1 Score performance metrics are computed and printed to provide insights into the model's performance.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Sample data\n",
    "X, y = load_sample_data()\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Evaluate model using the confusionMatrix function\n",
    "confusionMatrix(X, y, rf_model)\n",
    "```\n",
    "\n",
    "This would plot the confusion matrix for the random forest classifier and print its Recall, Precision, and F1 Score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e05c863-5457-406a-8d1a-4c44b3161666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the confusion matrix\n",
    "\n",
    "def confusionMatrix (X, y, model, split = 0.75, random_state = 42):\n",
    "    \n",
    "    # Splitting the data into a Testing and tarinign set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, train_size = split, random_state = random_state)\n",
    "    \n",
    "    #fit the training data to the algorithm\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Run the algorithm on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Prepare and plot the confusions matrix\n",
    "    \n",
    "    plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues)\n",
    "    print('\\033[1m'+'Performance Metrics' + '\\033[0m' + '\\nRecall: {:.2f}% \\nPrecision: {:.2f}%\\nF1 Score: {:.2f}%'.format(\n",
    "        recall_score(y_test, y_pred)*100, precision_score(y_test, y_pred)*100, f1_score(y_test, y_pred)*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85392c5e-fbb5-484e-be8d-8c846510f638",
   "metadata": {},
   "source": [
    "## Function: `shapValue` \n",
    "\n",
    "Compute and optionally visualize the SHAP (SHapley Additive exPlanations) values for a given machine learning model, which helps in understanding the impact of each feature on the model's predictions.\n",
    "<br><br>\n",
    "#### Parameters:\n",
    "\n",
    "- `X_train` : DataFrame, shape (n_samples, n_features)\n",
    "    - The training feature matrix. Used for creating the SHAP explainer object.\n",
    "<br><br>\n",
    "- `X_test` : DataFrame, shape (n_samples, n_features)\n",
    "    - The testing feature matrix. SHAP values are computed for this data.\n",
    "<br><br>\n",
    "- `model` : Scikit-learn classifier or regressor instance\n",
    "    - The machine learning model for which SHAP values are computed. Should implement a `predict` or `predict_proba` method.\n",
    "<br><br>\n",
    "- `K` : int, optional, default=25\n",
    "    - The number of instances sampled from `X_train` to approximate the SHAP values.\n",
    "<br><br>\n",
    "- `random_state` : int, optional, default=42\n",
    "    - Seed used by the random number generator during sampling.\n",
    "<br><br>\n",
    "- `plotting` : bool, optional, default=False\n",
    "    - If True, plots the SHAP summary and bar plots.\n",
    "<br><br>\n",
    "#### Returns:\n",
    "\n",
    "- `shap_dict` : dict\n",
    "    - A dictionary containing features as keys and their mean absolute SHAP values as values. It provides an understanding of the feature importances as determined by SHAP values.\n",
    "\n",
    "#### Function Behavior:\n",
    "\n",
    "1. Samples `K` data points from the training data (`X_train`).\n",
    "2. Prepares the model for SHAP explainer, ensuring compatibility by wrapping the `predict_proba` method.\n",
    "3. Creates a SHAP explainer object using the sampled data points.\n",
    "4. Computes the SHAP values for the test data (`X_test`).\n",
    "5. Constructs a dictionary of feature importances based on the mean absolute SHAP values.\n",
    "6. If `plotting` is True, visualizes the SHAP values using summary and bar plots.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test = train_test_split(X, random_state=42)\n",
    "\n",
    "# Create a random forest classifier and fit\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y)\n",
    "\n",
    "# Evaluate SHAP values and visualize\n",
    "shap_dict = shapValue(X_train, X_test, rf_model, plotting=True)\n",
    "\n",
    "# Print feature importances based on SHAP values\n",
    "print(shap_dict)\n",
    "```\n",
    "\n",
    "This example computes and visualizes SHAP values for a RandomForest classifier on the Iris dataset and then prints the feature importances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e454d1-3832-4ce6-b884-94e834cdc5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the Shap Value\n",
    "\n",
    "def shapValue(X_train, X_test, model,  K=25, random_state=42, plotting = False):\n",
    "    # Sample K data points from the training data\n",
    "    X_train_sample = shap.sample(X_train, K, random_state=random_state)\n",
    "    \n",
    "    # Define a wrapped predict_proba function\n",
    "    def wrappedPredict(data):\n",
    "       return model.predict_proba(data)\n",
    "\n",
    "    # Create the explainer object\n",
    "    try:\n",
    "        explainer = shap.KernelExplainer(model.predict_proba, X_train_sample)\n",
    "    except AttributeError:\n",
    "        explainer = shap.KernelExplainer(wrappedPredict, X_train_sample)\n",
    "    \n",
    "    # Compute SHAP values\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    \n",
    "    # Create an Explanation object\n",
    "    explainer = shap.Explanation(values=shap_values[0], \n",
    "                                 data=X_test.values, \n",
    "                                 feature_names=X_train.columns)\n",
    "    \n",
    "    # Calculate the mean absolute SHAP values for each feature\n",
    "    mean_shap_values = np.abs(shap_values[0]).mean(0)\n",
    "    \n",
    "    # Create a DataFrame for feature importance\n",
    "    feature_importance = pd.DataFrame(list(zip(X_train.columns, mean_shap_values)), columns=['col_name', 'feature_importance_values'])\n",
    "    feature_importance.sort_values(by=['feature_importance_values'], ascending=False, inplace=True)\n",
    "\n",
    "    # Create a dictionary to return\n",
    "    shap_dict = feature_importance.set_index('col_name')['feature_importance_values'].to_dict()\n",
    "    \n",
    "    # Plotting\n",
    "    if plotting:\n",
    "        shap.summary_plot(shap_values[0], X_test, X_train.columns.tolist())\n",
    "        shap.plots.bar(explainer, max_display=12)\n",
    "    \n",
    "    return shap_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387d48f-128a-438a-b8c1-d0ff3fe2ed7f",
   "metadata": {},
   "source": [
    "## Function: `localExplanation` \n",
    "\n",
    "Compute a local explanation of a given machine learning model's prediction using LIME (Local Interpretable Model-agnostic Explanations). It gives insight into how each feature contributes to a specific instance's prediction.\n",
    "<br><br>\n",
    "#### Parameters:\n",
    "\n",
    "- `X` : DataFrame, shape (n_samples, n_features)\n",
    "    - The feature matrix. Used for creating the LIME explainer object.\n",
    "<br><br>\n",
    "- `y` : array-like, shape (n_samples,)\n",
    "    - The target variable. Used for the LIME explainer's initialization.\n",
    "<br><br>\n",
    "- `model` : Scikit-learn classifier or regressor instance\n",
    "    - The machine learning model for which local explanations are computed. It should implement a `predict` or `predict_proba` method.\n",
    "<br><br>\n",
    "- `explainInstance` : int, optional, default=0\n",
    "    - The index of the instance in `X` for which the local explanation is computed.\n",
    "<br><br>\n",
    "- `plotting` : bool, optional, default=False\n",
    "    - If True, displays the LIME explanation plot in a notebook environment.\n",
    "<br><br>\n",
    "#### Returns:\n",
    "\n",
    "- `mean_lime_values` : dict\n",
    "    - A dictionary containing features as keys and their average LIME values as values for the explained instance.\n",
    "\n",
    "#### Function Behavior:\n",
    "\n",
    "1. Initializes a LIME explainer using the provided data (`X` and `y`).\n",
    "2. Defines a dictionary to store feature effects.\n",
    "3. Generates local explanations for the specified instance using the provided model.\n",
    "4. Aggregates feature effects for the explained instance.\n",
    "5. If `plotting` is True, visualizes the LIME explanations.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Create a random forest classifier and fit\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Generate local explanation for the first instance and visualize\n",
    "lime_dict = localExplanation(X, y, rf_model, explainInstance=0, plotting=True)\n",
    "\n",
    "# Print local feature importances based on LIME\n",
    "print(lime_dict)\n",
    "```\n",
    "\n",
    "In this example, a local explanation is generated for the first instance of the Iris dataset using a RandomForest classifier. The LIME visualizations are displayed, and the local feature importances are printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1402bf5c-de39-462e-9efa-e4a1b2fc0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for LIME values\n",
    "\n",
    "def localExplanation(X, y, model, explainInstance=0, plotting = False):\n",
    "    numberFeatures = X.shape[1]\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        X.values,\n",
    "        training_labels=y,\n",
    "        feature_names=X.columns.tolist(),\n",
    "        class_names=['0', '1'],\n",
    "        mode='classification'\n",
    "    )\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    feature_effects = {name: [] for name in feature_names}\n",
    "    \n",
    "    indices = [explainInstance]  # Just for the specific row\n",
    "   \n",
    "    for i in indices:\n",
    "        exp = explainer.explain_instance(\n",
    "            X.iloc[i].values,\n",
    "            model.predict_proba,\n",
    "            num_features=numberFeatures\n",
    "        )\n",
    "        \n",
    "        for feature_condition, effect in exp.as_list():\n",
    "            for feature_name in feature_names:\n",
    "                if feature_name in feature_condition:\n",
    "                    feature_effects[feature_name].append(effect)\n",
    "                    break\n",
    "                \n",
    "    mean_lime_values = {k: np.mean(v) if v else 0 for k, v in feature_effects.items()}\n",
    "    \n",
    "    #Plotting\n",
    "    if plotting:\n",
    "        exp.show_in_notebook()\n",
    "        print(\"Feature names from LIME:\", lime_values_dict.keys())\n",
    "    \n",
    "    return mean_lime_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a01409a-f4e1-4f47-9988-d61a6af8d5fc",
   "metadata": {},
   "source": [
    "## Function:`generateTreeMap` \n",
    "\n",
    "Visualize the impact of features on a machine learning model's prediction using a treemap. The size of each feature box in the treemap is determined by its SHAP value, while the color represents its LIME value.\n",
    "<br><br>\n",
    "#### Parameters:\n",
    "\n",
    "- `shap_values_storage` : dict\n",
    "    - A dictionary storing precomputed SHAP values for different models.\n",
    "<br><br>\n",
    "- `X` : DataFrame, shape (n_samples, n_features)\n",
    "    - The feature matrix.\n",
    "<br><br>\n",
    "- `y` : array-like, shape (n_samples,)\n",
    "    - The target variable.\n",
    "<br><br>\n",
    "- `model` : Scikit-learn classifier or regressor instance\n",
    "    - The machine learning model for which the treemap is generated.\n",
    "<br><br>\n",
    "- `modelName` : str or model object\n",
    "    - Name of the model or the model object itself. Used for fetching SHAP values from the storage.\n",
    "<br><br>\n",
    "- `split` : float, optional, default=0.75\n",
    "    - Fraction of the data to be used for training. The remaining data is used for testing.\n",
    "<br><br>\n",
    "- `K` : int, optional, default=25\n",
    "    - The number of instances sampled from `X_train` to approximate the SHAP values.\n",
    "<br><br>\n",
    "- `random_state` : int, optional, default=42\n",
    "    - Seed used by the random number generator.\n",
    "<br><br>\n",
    "- `explainInstance` : int, optional\n",
    "    - The index of the instance in `X` for which the LIME explanation is computed.\n",
    "<br><br>\n",
    "- `sortBy` : {'SHAP', 'LIME'}, optional, default='SHAP'\n",
    "    - Criteria for sorting features in the treemap.\n",
    "<br><br>\n",
    "- `shapPercentageCutoff` : float, optional, default=0\n",
    "    - Percentage threshold for filtering out features based on their SHAP values.\n",
    "<br><br>\n",
    "- `plottingSHAP` : bool, optional, default=False\n",
    "    - If True, displays a bar plot of mean SHAP values.\n",
    "<br><br>\n",
    "- `plottingLIME` : bool, optional, default=False\n",
    "    - If True, displays LIME's local explanation for a particular instance.\n",
    "<br><br>\n",
    "- `legend` : bool, optional, default=False\n",
    "    - If True, generates a legend matching feature names to numeric values and saves it as a CSV file.\n",
    "<br><br>\n",
    "- `plotName` : str, optional, default=\"treemapPlot.png\"\n",
    "    - Name of the file where the treemap is saved.\n",
    "<br><br>\n",
    "#### Returns:\n",
    "\n",
    "- No return value. The function directly visualizes the treemap and optionally saves it as an image.\n",
    "\n",
    "#### Function Behavior:\n",
    "\n",
    "1. The function splits the dataset into training and testing sets.\n",
    "2. Fetches precomputed SHAP values or computes them if needed.\n",
    "3. Calculates LIME values for the specified instance.\n",
    "4. Normalizes LIME values for color mapping and calculates SHAP percentages.\n",
    "5. Filters, sorts, and organizes features based on the provided criteria.\n",
    "6. Plots and saves the treemap.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Create a random forest classifier and fit\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Compute SHAP values (assuming shapValue function is defined)\n",
    "shap_values_storage = {\n",
    "    'RandomForestClassifier': shapValue(X_train, X_test, rf_model)\n",
    "}\n",
    "\n",
    "# Generate treemap visualization\n",
    "generateTreeMap(shap_values_storage, X, y, rf_model, modelName='RandomForestClassifier', explainInstance=0, plottingSHAP=True, plottingLIME=True, legend=True)\n",
    "```\n",
    "\n",
    "In this example, a treemap visualization is generated for a RandomForest classifier trained on the Iris dataset. The visualization showcases feature importances and effects using SHAP and LIME values, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee89b1-5512-46d8-8570-d66b27a6ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTreeMap(X, y, model, split = 0.75, K = 25, random_state = 42, explainInstance=None, \n",
    "                    sortBy = 'SHAP', shapPercentageCutoff = 0, plottingSHAP = True, plottingLIME = True):\n",
    "    \n",
    "    # Splitting the data into a Testing and tarinign set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify = y, \n",
    "        train_size = split, \n",
    "        random_state = random_state)\n",
    "    \n",
    "    # Get the absolute mean SHAP values\n",
    "    shap_values_dict = shapValue(X_train, X_test, model, K, random_state, plotting = plottingSHAP)  # Assuming shapValue returns absolute mean SHAP values\n",
    "    \n",
    "    # Get the feature names from the dataset\n",
    "    featureNames = X_train.columns.tolist()\n",
    "    \n",
    "    # Get the LIME values\n",
    "    lime_values_dict = localExplanation(X, y, model, explainInstance=explainInstance, plotting = plottingLIME)\n",
    "    \n",
    "    # Make sure shap_values and lime_values are ordered the same way\n",
    "    shap_values = [shap_values_dict[feature] for feature in featureNames]\n",
    "    lime_values = [lime_values_dict[feature] for feature in featureNames]\n",
    "    \n",
    "    # Normalize LIME values to fit into a color map\n",
    "    lime_max_abs = max(max(lime_values), abs(min(lime_values)))\n",
    "    norm = mcolors.Normalize(-lime_max_abs, lime_max_abs)  # I've set the range from -lime_max_abs to lime_max_abs\n",
    "    colors = [plt.cm.RdYlGn(norm(value)) for value in lime_values]\n",
    "    \n",
    "    # Calculate SHAP value percentages\n",
    "    total_shap = sum(shap_values)\n",
    "    shap_percentages = [(shap / total_shap) * 100 for shap in shap_values]\n",
    "    features_with_percentages = [f'{feature} ({percentage:.2f}%)' for feature, percentage in zip(featureNames, shap_percentages)]\n",
    "    \n",
    "    \n",
    "    # Zip all lists together\n",
    "    zipped_lists = zip(shap_values, lime_values, featureNames, shap_percentages)\n",
    "\n",
    "    # Sort by SHAP values (you can also sort by absolute LIME values by changing the key)\n",
    "    if sortBy == 'SHAP':\n",
    "        sorted_zipped_lists = sorted(zipped_lists, key=lambda x: x[0], reverse=True)\n",
    "    elif sortBy == 'LIME':\n",
    "        sorted_zipped_lists = sorted(zipped_lists, key=lambda x: x[1], reverse=True)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid value for sortBy. Choose 'SHAP' or 'LIME'.\")\n",
    "    \n",
    "    # Unzip sorted lists\n",
    "    sorted_shap_values, sorted_lime_values, sorted_feature_names, sorted_shap_percentages = zip(*sorted_zipped_lists)\n",
    "\n",
    "    # Filter features with SHAP percentage higher than the cutoff\n",
    "    filtered_indices = [i for i, perc in enumerate(sorted_shap_percentages) if perc >= shapPercentageCutoff]\n",
    "    filtered_shap_values = [sorted_shap_values[i] for i in filtered_indices]\n",
    "    filtered_lime_values = [sorted_lime_values[i] for i in filtered_indices]\n",
    "    filtered_feature_names = [sorted_feature_names[i] for i in filtered_indices]\n",
    "    filtered_shap_percentages = [sorted_shap_percentages[i] for i in filtered_indices]\n",
    "\n",
    "    features_with_percentages = [f'{name} ({perc:.2f}%)' for name, perc in zip(filtered_feature_names, filtered_shap_percentages)]\n",
    "\n",
    "    # Normalize LIME values for the color map\n",
    "    lime_max_abs = max(max(filtered_lime_values), abs(min(filtered_lime_values)))\n",
    "    norm = mcolors.Normalize(-lime_max_abs, lime_max_abs)\n",
    "    filtered_colors = [plt.cm.RdYlGn(norm(value)) for value in filtered_lime_values]\n",
    "    \n",
    "    # Create a treemap\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    squarify.plot(sizes=filtered_shap_values, label=features_with_percentages, color=filtered_colors, alpha=0.7, edgecolor=\"k\", linewidth=1, pad = 1)\n",
    "    plt.title(\"Feature Importance and Effect\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Add color bar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.RdYlGn, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ticks=np.linspace(-lime_max_abs, lime_max_abs, 5))\n",
    "    cbar.set_label('Normalized LIME value')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10667773-82e4-4365-b054-570105a41988",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Functions neede in the above functions, but not intended to be for general use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95719d5-92d5-471d-b029-75bc3e2d599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to show SHAp value in generateTreeMap\n",
    "def plot_mean_shap_from_dict(shap_values_dict):\n",
    "    sorted_items = sorted(shap_values_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    features, values = zip(*sorted_items)\n",
    "    \n",
    "    plt.figure(figsize=(10, len(features)*0.4))\n",
    "    plt.barh(features, values, color='royalblue')\n",
    "    plt.xlabel(\"Mean SHAP Value\")\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.gca().invert_yaxis()  # to display the most important feature at the top\n",
    "    plt.savefig('shap_mean_plot.png', dpi=800, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48037750-12a9-4ade-9ec7-9141a2e5d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate NHHI\n",
    "def calc_NHHI(shap_dict):\n",
    "    total = sum(shap_dict.values())\n",
    "    squared_proportions = [(x / total) ** 2 for x in shap_dict.values()]\n",
    "    NHHI = sum(squared_proportions)\n",
    "    return NHHI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45e8d3-8afb-474e-93e4-ea85bbe7f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate composite metrics for multiple models and store SHAP values\n",
    "def calc_composite_and_store_shap(X_train, X_test, y_test, models, metric=\"f1\", alpha=0.5, K=25, random_state=42, T=None):\n",
    "    NHHIs = []\n",
    "    NHHIs_dict = {}  # Dictionary to store NHHI values for each model\n",
    "    composite_metrics = {}\n",
    "    shap_values_storage = {}\n",
    "    \n",
    "    # Calculate the NHHI for each model and store SHAP values\n",
    "    for model in models:\n",
    "        shap_dict = shapValue(X_train, X_test, model, K=K, random_state=random_state, plotting=False)\n",
    "        NHHI = calc_NHHI(shap_dict)\n",
    "        NHHIs.append(NHHI)\n",
    "        \n",
    "        model_name = str(model).split(\"(\")[0]\n",
    "        shap_values_storage[model_name] = shap_dict\n",
    "        \n",
    "        NHHIs_dict[model_name] = NHHI  # Store the NHHI value for the current model\n",
    "    \n",
    "    # If T is not provided, calculate the mean NHHI\n",
    "    if T is None:\n",
    "        T = np.mean(NHHIs)\n",
    "    \n",
    "    # Calculate the composite metric for each model\n",
    "    for i, model in enumerate(models):\n",
    "        NHHI = NHHIs[i]\n",
    "        \n",
    "        # Get model predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Get chosen metric (Recall, Precision, F1)\n",
    "        if metric == \"f1\":\n",
    "            chosen_metric = f1_score(y_test, y_pred)\n",
    "        elif metric == \"precision\":\n",
    "            chosen_metric = precision_score(y_test, y_pred)\n",
    "        elif metric == \"recall\":\n",
    "            chosen_metric = recall_score(y_test, y_pred)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid metric choice. Please choose 'f1', 'precision', or 'recall'.\")\n",
    "        \n",
    "        # Calculate penalty term based on deviation from ideal NHHI (T)\n",
    "        P = abs(NHHI - T)\n",
    "        \n",
    "        # Calculate composite metric\n",
    "        composite_metric = chosen_metric * (1 - alpha * P)\n",
    "        model_name = str(model).split(\"(\")[0]\n",
    "        \n",
    "        composite_metrics[model_name] = composite_metric\n",
    "        \n",
    "        # Sort the composite metrics\n",
    "        composite_metrics = OrderedDict(sorted(composite_metrics.items(), key=lambda x: x[1], reverse=True))\n",
    "        \n",
    "        # Sort the NHHI\n",
    "        NHHIs_dict =  OrderedDict(sorted(NHHIs_dict.items(), key=lambda x: x[1], reverse=False))\n",
    "    \n",
    "    return composite_metrics, shap_values_storage, NHHIs_dict  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09691a3-e393-4874-9a62-aedf4c439450",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "Chen, T. and Guestrin, C., 2016. XGBoost: A Scalable Tree Boosting System. San Francisco, California, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 785 - 794.\n",
    "\n",
    "Harris, C. R. et al., 2020. Array programming with NumPy. Nature, 585(7825), pp. 357 - 362.\n",
    "\n",
    "Hunter, J. D., 2007. Matplotlib: a 2D graphics environment. Computing in Science & Engineering, 9(3), pp. 90 - 95.\n",
    "\n",
    "Laserson, U., 2021. Pure Python implementation of the squarify treemap layout algorithm. s.l.:s.n.\n",
    "\n",
    "Lundberg, S. M. and Lee, S.-I., 2017. A unified Approach to Interpreting Model Predictions. Long Beach, CA, Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS), pp. 4768 - 4777.\n",
    "\n",
    "McKinney, W., 2010. Data Structures for Statisitcal Computing in Python. s.l., Proceedings of the 9th Python in Science Conference, pp. 56 - 61.\n",
    "\n",
    "Pedregosa, F. et al., 2011. Scikit-learn: Machoine learning in Python. Journal of Machine Learning Research, Volume 12, pp. 2825 - 2830.\n",
    "\n",
    "Ribeiro, M. T., Singh, S. and Guestrin, C., 2016. \"Why Should I Trust You?\" Explaining the Predictions of Any Classifier. San Francisco, California, KDD '16: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1135 - 1144.\n",
    "\n",
    "Waskom, M. L., 2021. seaborn: statistical data visualization. Journal of Open Source Software, 6(60), p. 3021\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
